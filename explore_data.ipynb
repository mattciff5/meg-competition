{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb86c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/miniconda3/envs/huggin/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pnpl.datasets import LibriBrainSpeech\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a04f09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/srv/nfs-data/sisko/storage/libribrain\"\n",
    "train_keys = []\n",
    "val_keys = [(\"0\", \"11\", \"Sherlock1\", \"2\")] \n",
    "test_keys = [(\"0\", \"12\", \"Sherlock1\", \"2\")]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e956e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherlock1: solo le prime 10 sessioni run1 → training\n",
    "for sess_id in range(1,11):\n",
    "    train_keys.append((\"0\", str(sess_id), \"Sherlock1\", \"1\"))\n",
    "\n",
    "# Sherlock2–4: 12 sessioni run1 → training\n",
    "for subject_idx, stim_name in zip(range(1, 2), [\"Sherlock2\"]):\n",
    "    for sess_id in range(1,13):\n",
    "        train_keys.append((0, str(sess_id), stim_name, \"1\"))\n",
    "\n",
    "# Sherlock5: 15 sessioni run1 → training\n",
    "# for sess_id in range(1,16):\n",
    "#     train_keys.append((\"0\", str(sess_id), \"Sherlock5\", \"1\"))\n",
    "\n",
    "# # Sherlock6: 14 sessioni run1 → training\n",
    "# for sess_id in range(1,15):\n",
    "#     train_keys.append((\"0\", str(sess_id), \"Sherlock6\", \"1\"))\n",
    "\n",
    "# # Sherlock7: 14 sessioni run1 → training\n",
    "# for sess_id in range(1,15):\n",
    "#     train_keys.append((\"0\", str(sess_id), \"Sherlock7\", \"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db25ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LibriBrainSpeech(\n",
    "    data_path=f\"{base_path}/data/\",\n",
    "    include_run_keys=train_keys,\n",
    "    tmin=0.0,\n",
    "    tmax=0.8,\n",
    "    preload_files=True\n",
    ")\n",
    "\n",
    "val_dataset = LibriBrainSpeech(\n",
    "    data_path=f\"{base_path}/data/\",\n",
    "    include_run_keys=val_keys,\n",
    "    tmin=0.0,\n",
    "    tmax=0.8,\n",
    "    preload_files=True\n",
    ")\n",
    "\n",
    "test_dataset = LibriBrainSpeech(\n",
    "    data_path=f\"{base_path}/data/\",\n",
    "    include_run_keys=test_keys,\n",
    "    tmin=0.0,\n",
    "    tmax=0.8,\n",
    "    preload_files=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bae284e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 35263\n",
      "Number of validation samples: 1671\n",
      "Number of test samples: 1772\n"
     ]
    }
   ],
   "source": [
    "num_workers = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa125cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([32, 306, 200])\n",
      "Batch label shape: torch.Size([32, 200])\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(train_loader))\n",
    "inputs, labels = first_batch\n",
    "print(\"Batch input shape:\", inputs.shape)\n",
    "print(\"Batch label shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "097e5efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset:\n",
      "Train data contains 85516 samples\n",
      "Validation data contains 1671 samples\n",
      "Test data contains 1772 samples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([32, 23, 200])\n",
      "Batch label shape: torch.Size([32])\n",
      "\n",
      "Single sample input shape: torch.Size([23, 200])\n",
      "Single sample label is just a single value now!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import platform\n",
    "\n",
    "\n",
    "SENSORS_SPEECH_MASK = [18, 20, 22, 23, 45, 120, 138, 140, 142, 143, 145,\n",
    "                       146, 147, 149, 175, 176, 177, 179, 180, 198, 271, 272, 275]\n",
    "\n",
    "class FilteredDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        dataset: LibriBrain dataset.\n",
    "        limit_samples (int, optional): If provided, limits the length of the dataset to this\n",
    "                          number of samples.\n",
    "        speech_silence_only (bool, optional): If True, only includes segments that are either\n",
    "                          purely speech or purely silence (with additional balancing).\n",
    "        apply_sensors_speech_mask (bool, optional): If True, applies a fixed sensor mask to the sensor\n",
    "                          data in each sample.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 limit_samples=None,\n",
    "                 disable=False,\n",
    "                 apply_sensors_speech_mask=True):\n",
    "        self.dataset = dataset\n",
    "        self.limit_samples = limit_samples\n",
    "        self.apply_sensors_speech_mask = apply_sensors_speech_mask\n",
    "\n",
    "        # These are the sensors we identified:\n",
    "        self.sensors_speech_mask = SENSORS_SPEECH_MASK\n",
    "\n",
    "        self.balanced_indices = list(range(len(dataset.samples)))\n",
    "        # Shuffle the indices\n",
    "        self.balanced_indices = random.sample(self.balanced_indices, len(self.balanced_indices))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the filtered dataset.\"\"\"\n",
    "        if self.limit_samples is not None:\n",
    "            return self.limit_samples\n",
    "        return len(self.balanced_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Map index to the original dataset using balanced indices\n",
    "        original_idx = self.balanced_indices[index]\n",
    "        if self.apply_sensors_speech_mask:\n",
    "            sensors = self.dataset[original_idx][0][self.sensors_speech_mask]\n",
    "        else:\n",
    "            sensors = self.dataset[original_idx][0][:]\n",
    "        label_from_the_middle_idx = self.dataset[original_idx][1].shape[0] // 2\n",
    "        return [sensors, self.dataset[original_idx][1][label_from_the_middle_idx]]\n",
    "\n",
    "\n",
    "print(\"Filtered dataset:\")\n",
    "train_data_filtered = FilteredDataset(train_dataset)\n",
    "train_loader_filtered = DataLoader(train_data_filtered, batch_size=32, shuffle=True, num_workers=num_workers)\n",
    "print(f\"Train data contains {len(train_data_filtered)} samples\")\n",
    "\n",
    "val_data_filtered = FilteredDataset(val_dataset)\n",
    "val_loader_filtered = DataLoader(val_data_filtered, batch_size=32, shuffle=False, num_workers=num_workers)\n",
    "print(f\"Validation data contains {len(val_data_filtered)} samples\")\n",
    "\n",
    "test_data_filtered = FilteredDataset(test_dataset)\n",
    "test_loader_filtered = DataLoader(test_data_filtered, batch_size=32, shuffle=False, num_workers=num_workers)\n",
    "print(f\"Test data contains {len(test_data_filtered)} samples\\n\")\n",
    "\n",
    "# Let's look at the first batch:\n",
    "first_batch = next(iter(train_loader_filtered))\n",
    "inputs, labels = first_batch\n",
    "print(\"Batch input shape:\", inputs.shape)\n",
    "print(\"Batch label shape:\", labels.shape)\n",
    "\n",
    "first_input = inputs[0]\n",
    "first_label = labels[0]\n",
    "print(\"\\nSingle sample input shape:\", first_input.shape)\n",
    "print(\"Single sample label is just a single value now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9da8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910dd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a638b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
